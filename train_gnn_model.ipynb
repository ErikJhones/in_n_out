{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeaa90d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/Downloads/lepra/.env/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from models import DeepVGAE\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.datasets.planetoid import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device_string = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device_string)\n",
    "\n",
    "seed = 42\n",
    "dataset_name = \"cora\"\n",
    "gnn = 'PEG'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "dataset = Planetoid(\"datasets\", dataset_name, transform=T.NormalizeFeatures())\n",
    "data = dataset[0]#.to(device)\n",
    "all_edge_index = data.edge_index\n",
    "data = train_test_split_edges(data, 0.05, 0.1)\n",
    "\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc2f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning embedding vectors...\n",
      "Learning embedding vectors done!\n",
      "total parameters: 23413\n",
      "Epoch: 010, Loss: 9.1198, Val: 0.1775, Test: 0.1115\n",
      "Epoch: 020, Loss: 7.0931, Val: 0.1775, Test: 0.1115\n",
      "Epoch: 030, Loss: 5.1068, Val: 0.1938, Test: 0.1241\n",
      "Epoch: 040, Loss: 3.1091, Val: 0.2622, Test: 0.2006\n",
      "Epoch: 050, Loss: 1.2857, Val: 0.6040, Test: 0.6238\n",
      "Epoch: 060, Loss: 0.4644, Val: 0.7972, Test: 0.8486\n",
      "Epoch: 070, Loss: 0.3294, Val: 0.8216, Test: 0.8687\n",
      "Epoch: 080, Loss: 0.2533, Val: 0.8378, Test: 0.8821\n",
      "Epoch: 090, Loss: 0.2160, Val: 0.8424, Test: 0.8822\n",
      "Epoch: 100, Loss: 0.2104, Val: 0.8456, Test: 0.8851\n",
      "Epoch: 110, Loss: 0.1948, Val: 0.8508, Test: 0.8926\n",
      "Epoch: 120, Loss: 0.1872, Val: 0.8542, Test: 0.8972\n",
      "Epoch: 130, Loss: 0.1760, Val: 0.8556, Test: 0.8990\n",
      "Epoch: 140, Loss: 0.1693, Val: 0.8563, Test: 0.8995\n",
      "Epoch: 150, Loss: 0.1632, Val: 0.8570, Test: 0.9001\n",
      "Epoch: 160, Loss: 0.1582, Val: 0.8578, Test: 0.9010\n",
      "Epoch: 170, Loss: 0.1520, Val: 0.8584, Test: 0.9016\n",
      "Epoch: 180, Loss: 0.1437, Val: 0.8589, Test: 0.9021\n",
      "Epoch: 190, Loss: 0.1380, Val: 0.8590, Test: 0.9023\n",
      "Epoch: 200, Loss: 0.1378, Val: 0.8591, Test: 0.9025\n",
      "Epoch: 210, Loss: 0.1340, Val: 0.8592, Test: 0.9029\n",
      "Epoch: 220, Loss: 0.1247, Val: 0.8594, Test: 0.9034\n",
      "Epoch: 230, Loss: 0.1206, Val: 0.8594, Test: 0.9034\n",
      "Epoch: 240, Loss: 0.1156, Val: 0.8596, Test: 0.9039\n",
      "Epoch: 250, Loss: 0.1156, Val: 0.8596, Test: 0.9038\n",
      "Epoch: 260, Loss: 0.1120, Val: 0.8597, Test: 0.9039\n",
      "Epoch: 270, Loss: 0.1095, Val: 0.8597, Test: 0.9039\n",
      "Epoch: 280, Loss: 0.1056, Val: 0.8598, Test: 0.9037\n",
      "Epoch: 290, Loss: 0.1031, Val: 0.8598, Test: 0.9038\n",
      "Epoch: 300, Loss: 0.0987, Val: 0.8598, Test: 0.9038\n",
      "Epoch: 310, Loss: 0.0930, Val: 0.8598, Test: 0.9038\n",
      "Epoch: 320, Loss: 0.0932, Val: 0.8598, Test: 0.9038\n",
      "Epoch: 330, Loss: 0.0916, Val: 0.8598, Test: 0.9038\n",
      "Epoch: 340, Loss: 0.0880, Val: 0.8598, Test: 0.9038\n",
      "Epoch: 350, Loss: 0.0895, Val: 0.8598, Test: 0.9038\n",
      "Epoch: 360, Loss: 0.0824, Val: 0.8598, Test: 0.9038\n",
      "Epoch: 370, Loss: 0.0837, Val: 0.8598, Test: 0.9038\n",
      "Epoch: 380, Loss: 0.0797, Val: 0.8598, Test: 0.9038\n",
      "Epoch: 390, Loss: 0.0800, Val: 0.8598, Test: 0.9038\n",
      "Epoch: 400, Loss: 0.0807, Val: 0.8598, Test: 0.9038\n"
     ]
    }
   ],
   "source": [
    "if gnn == 'VGAE':\n",
    "    enc_in_channels = data.x.shape[1]\n",
    "    enc_hidden_channels = 32\n",
    "    enc_out_channels = 16\n",
    "\n",
    "    lr = 0.01\n",
    "    epochs = 400\n",
    "    model = DeepVGAE(enc_in_channels, enc_hidden_channels, enc_out_channels).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(data.x, data.train_pos_edge_index, all_edge_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            model.eval()\n",
    "            roc_auc, ap = model.single_test(data.x,\n",
    "                                            data.train_pos_edge_index,\n",
    "                                            data.test_pos_edge_index,\n",
    "                                            data.test_neg_edge_index)\n",
    "            print(\"Epoch {} - Loss: {} ROC_AUC: {} Precision: {}\".format(epoch+1, loss.cpu().item(), roc_auc, ap))\n",
    "\n",
    "elif gnn == 'PEG':\n",
    "    #Build train matrix for PE preparation\n",
    "    from torch_geometric.utils import to_networkx\n",
    "\n",
    "    import copy\n",
    "    train_graph = copy.deepcopy(data)\n",
    "    train_graph.edge_index = data.train_pos_edge_index\n",
    "    G = to_networkx(train_graph)\n",
    "    del train_graph\n",
    "\n",
    "    from peg_utils import *\n",
    "\n",
    "    model_emb = DeepWalk(G,walk_length=80, num_walks=10,workers=1)#init model\n",
    "    model_emb.train(embed_size = 10)# train model\n",
    "    emb = model_emb.get_embeddings()# get embedding vectors\n",
    "    embeddings = []\n",
    "    for i in range(len(emb)):\n",
    "        embeddings.append(emb[i])\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    x = data.x\n",
    "    pos = torch.tensor(embeddings)\n",
    "    pos = pos.type(torch.FloatTensor)\n",
    "    x = x.type(torch.FloatTensor)\n",
    "\n",
    "    for run in range(1):\n",
    "        total_class = 16\n",
    "\n",
    "        model = Net(in_feats_dim = data.num_features, pos_dim = 128, hidden_dim = total_class, node_dim=data.x.shape[1])\n",
    "        data = data.to(device) \n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr= 0.01)\n",
    "        print(f'total parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "        best_val_perf = test_perf = 0\n",
    "        for epoch in range(1, 401):\n",
    "            train_loss = train_peg(model, optimizer, pos, data, x, device)\n",
    "            val_perf, tmp_test_perf = test_peg(model, pos, data, x, device)\n",
    "            if val_perf > best_val_perf:\n",
    "                best_val_perf = val_perf\n",
    "                test_perf = tmp_test_perf\n",
    "            log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "            if epoch % 10 == 0:\n",
    "                print(log.format(epoch, train_loss, best_val_perf, test_perf))\n",
    "else:\n",
    "    print('ERROR: GNN donÂ´t exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58729c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"models/\"\n",
    "torch.save(\n",
    "    {\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "        },\n",
    "        path + gnn + '_'+dataset_name+ '_seed_' + str(seed) + '.pth.tar',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c7b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
